@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  date = {2016-08-13},
  eprint = {1603.02754},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  url = {http://arxiv.org/abs/1603.02754},
  urldate = {2023-02-19},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/98PTEDEK/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;/Users/derekberger/Zotero/storage/8X7CRFF5/1603.html}
}

@misc{kadraWelltunedSimpleNets2021,
  title = {Well-Tuned {{Simple Nets Excel}} on {{Tabular Datasets}}},
  author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  date = {2021-11-05},
  number = {arXiv:2106.11189},
  eprint = {2106.11189},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.11189},
  url = {http://arxiv.org/abs/2106.11189},
  urldate = {2023-02-19},
  abstract = {Tabular datasets are the last "unconquered castle" for deep learning, with traditional ML methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters. We empirically assess the impact of these regularization cocktails for MLPs in a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/USK36U9V/Kadra et al. - 2021 - Well-tuned Simple Nets Excel on Tabular Datasets.pdf;/Users/derekberger/Zotero/storage/3UXP5XUT/2106.html}
}

@article{pedregosaScikitlearnMachineLearning2011b,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v12/pedregosa11a.html},
  urldate = {2023-02-19},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {/Users/derekberger/Zotero/storage/HXXHHK7G/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@inproceedings{Williams:161322,
  title = {Using the Nystroem Method to Speed up Kernel Machines},
  author = {Williams, Christopher and Seeger, Matthias},
  date = {2001},
  series = {Proceedings of the 14th {{Annual Conference}} on {{Neural Information Processing Systems}}},
  pages = {682--688},
  url = {http://infoscience.epfl.ch/record/161322}
}
